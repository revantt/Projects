{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "imdb.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16501718269104802468\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1547036262\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12708826497356550644\n",
      "physical_device_desc: \"device: 0, name: GeForce 930MX, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imdb\n",
    "x_train, y_train = imdb.load_data(train=True)\n",
    "x_test, y_test = imdb.load_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words=[]\n",
    "for i in x_test+x_train:\n",
    "    total_words.append(i)\n",
    "num_words=list(set(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49582\n"
     ]
    }
   ],
   "source": [
    "print(len(num_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(x_train+x_test)\n",
    "x_train_token=tokenizer.texts_to_sequences(x_train)\n",
    "x_test_token=tokenizer.texts_to_sequences(x_test)\n",
    "max_tokens=600\n",
    "x_train_pad = pad_sequences(x_train_token, maxlen=max_tokens,padding='pre', truncating='pre')\n",
    "x_test_pad = pad_sequences(x_test_token, maxlen=max_tokens,padding='pre', truncating='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=len(num_words),output_dim=8,input_length=max_tokens,name='layer_embedding'))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=16))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(1e-3),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23750 samples, validate on 1250 samples\n",
      "Epoch 1/1\n",
      "23750/23750 [==============================] - 802s - loss: 0.6849 - acc: 0.5453 - val_loss: 0.7664 - val_acc: 0.4184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x23b07885f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Now lets train the model#Now let \n",
    "model.fit(x_train_pad,y_train,validation_split=0.05,epochs=1,batch_size=100)\n",
    "#Performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54294997]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "tr=Translator()\n",
    "text=tr.translate('안녕하세요.')\n",
    "test=[text.text]\n",
    "test_token=tokenizer.texts_to_sequences(test)\n",
    "tokens_pad = pad_sequences(test_token, maxlen=600,padding='pre', truncating='pre')\n",
    "model.predict(tokens_pad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.641119]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=tr.translate('この文章は日本語で書かれました')\n",
    "test=[text.text]\n",
    "test_token=tokenizer.texts_to_sequences(test)\n",
    "tokens_pad = pad_sequences(test_token, maxlen=600,padding='pre', truncating='pre')\n",
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68429518]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=tr.translate('Tiu frazo estas skribita en Esperanto.')\n",
    "test=[text.text]\n",
    "test_token=tokenizer.texts_to_sequences(test)\n",
    "tokens_pad = pad_sequences(test_token, maxlen=600,padding='pre', truncating='pre')\n",
    "model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
